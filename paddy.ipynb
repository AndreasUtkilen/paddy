{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31a98e39",
      "metadata": {},
      "source": [
        "# Assigment 3\n",
        "\n",
        "## Task 1 - Paddy Disease Detection\n",
        "\n",
        "As stated in the assignment, we will create a model to detect paddy diseases using the dataset provided. I started by looking and implementing the code that Jeremy Howard provided, and managed to get similar results to the ones he got, although the provided compute is not suitable for his models and resulted in a very long training time. I therefore started to play with the model and tried to improve the results while keeping the training time at tolerable level.\n",
        "\n",
        "I initially did a test run of what Jeremy Howard had done, just to see if I could replicate his results. After several hours of training on a Google Colab notebook, which offers 16 GB of VRAM compared to our 8 GB at the computer lab, I got a similar result, but was left with no more compute tokens. I therefore realised that in order to continue working on this task with limited compute I had to make some changes.\n",
        "\n",
        "The first thing I did was to try different architectures. I started with the Resnet50 architecture, which is a very popular, but now an old architecture for image classification tasks, and to reduce the training time, I used a smaller input size of 224x224. The results were not very good, with a validation accuracy of around 0.8. I then tried the Resnet101 architecture, which is a deeper version of Resnet50, and the results improved slightly, with a validation accuracy of around 0.86. I could have tried to improve these resnet models more, with more augmentations and fine-tuning the hyperparameters, but I knew that Mr. Howard has used different architectures, such as ConvNext and ViT.\n",
        "\n",
        "I therefore tried the ConvNext Small 22k and found that it performed much better than a Resnet101 and similar architectures. This is likely due to the fact that ConvNext is a modernized convolutional neural network inspired by the transformer era (like ViT), while it keeps the efficiency of CNNs and that the Convnext model is pretrained on ImageNet-22K (22,000 classes), and Resnet is usually pretrained on ImageNet-1K (1,000 classes). The ConvNext has a tendency to build stronger hierarchical features due to its modern block structure and larger receptive fields, and since the disease is not very visible, the model needs to learn more complex features.\n",
        "\n",
        "I then tried the ConvNext Small 22k and found that it performed much better than a Resnet101 or similar architectures. This is likely due to the fact that ConvNext is a modernized convolutional neural network inspired by the transformer era (like ViT), while it keeps the efficiency of CNNs and that the Convnext model is pretrained on ImageNet-22K (22,000 classes), and Resnet is usually pretrained on ImageNet-1K (1,000 classes). The ConvNext has a tendency to build stronger hierarchical features due to its modern block structure and larger receptive fields, and since the disease is not very visible, the model needs to learn more complex features.\n",
        "\n",
        "To try to make the model even better I created an ensemble of Resnet models and ConvNext models with different hyperparameters and augmentations. The results were very good, with a score of around 0.97846 on Kaggle, while still keeping the training time to a reasonable level (34 min, 8 GB GPU memory). The first ensemble looked like this:\n",
        "\n",
        "| Arch                  | Image size | Epochs | Augmentations   | Error rate | Training time (approx) |\n",
        "| --------------------- | ---------- | ------ | --------------- | ---------- | ---------------------- |\n",
        "| resnet50              | 224        | 10     | scale (min 75%) | 0.144      | 7 min                  |\n",
        "| resnet101             | 224        | 10     | scale (min 75%) | 0.142      | 10 min                 |\n",
        "| convnext_small_in22k  | 224        | 10     | scale (min 75%) | 0.028      | 11 min                 |\n",
        "| vit_small_patch16_224 | 224        | 10     | scale (min 75%) | 0.026      | 6 min                  |\n",
        "\n",
        "| Kaggle score          | 0.97846 |\n",
        "| --------------------- | ------- |\n",
        "\n",
        "The ensemble works similarly to Jeremy's model, where the predictions of each model are averaged to get the final prediction.\n",
        "\n",
        "After realizing that the resnets were not performing as well as the ConvNext and ViT models, I decided to remove them from the ensemble and only keep the ConvNext and ViT models. The next ensemble turned out like this:\n",
        "\n",
        "| Arch                  | Image size | Epochs | Augmentations   | Error rate | Training time (approx) |\n",
        "| --------------------- | ---------- | ------ | --------------- | ---------- | ---------------------- |\n",
        "| convnext_small_in22k  | 224        | 10     | scale (min 75%) | 0.024      | 11 min                 |\n",
        "| vit_small_patch16_224 | 224        | 10     | scale (min 75%) | 0.029      | 6 min                  |\n",
        "\n",
        "| Kaggle score          | 0.98306 |\n",
        "| --------------------- | ------- |\n",
        "\n",
        "Which resulted in a score of 0.98306 on Kaggle, which is slightly better. An ensemble with only two models is not really enough when the descision is based on the average of the predictions though, so I added two more variants of the ConvNext model and the ViT model with different augmentations and hyperparameters. The improved ensemble looked like this:\n",
        "\n",
        "| Arch                  | Image size | Epochs | Augmentations   | Error rate | Training time (approx) |\n",
        "| --------------------- | ---------- | ------ | --------------- | ---------- | ---------------------- |\n",
        "| convnext_small_in22k  | 299        | 10     | scale (min 75%) | 0.024      | 16 min                 |\n",
        "| convnext_small_in22k  | 224        | 10     | scale (min 75%) | 0.024      | 11 min                 |\n",
        "| vit_small_patch16_224 | 224        | 10     | scale (min 75%) | 0.029      | 6 min                  |\n",
        "\n",
        "| Kaggle score          | 0.98385 |\n",
        "| --------------------- | ------- |\n",
        "\n",
        "This resulted in a score of 0.98385 on Kaggle. The training time was around 30 minutes, and the GPU memory usage was around 8 GB.\n",
        "\n",
        "As a final test, I added a nother ConvNext model with a different input size of 320. Which resulted in a slightly higher score (0.98539) on Kaggle, but with the downside of a substantially longer training time of around 1 hour and 34 minutes. The final ensemble looked like this:\n",
        "\n",
        "| Arch                  | Image size | Epochs | Augmentations   | Error rate | Training time (approx) |\n",
        "| --------------------- | ---------- | ------ | --------------- | ---------- | ---------------------- |\n",
        "| convnext_small_in22k  | 320        | 10     | scale (min 75%) | 0.021      | 60 min                 |\n",
        "| convnext_small_in22k  | 299        | 10     | scale (min 75%) | 0.024      | 16 min                 |\n",
        "| convnext_small_in22k  | 224        | 10     | scale (min 75%) | 0.025      | 11 min                 |\n",
        "| vit_small_patch16_224 | 244        | 10     | scale (min 75%) | 0.032      | 6 min                  |\n",
        "\n",
        "| Kaggle score          | 0.98539 |\n",
        "| --------------------- | ------- |\n",
        "\n",
        "I spent a lot of time trying out if I could improve the GPU/CPU usage by rescaling the whole dataset to the desired sizes before starting any training, without much success. The resize function and scale augmentations in Fast AI are not the bottle neck of the training. \n",
        "\n",
        "In terms of other augmentations, I used Fast AI's standard augmentations which is defined as follows:\n",
        "\n",
        "| Augmentation                          | Value/Setting         |\n",
        "|----------------------------------------|-----------------------|\n",
        "| Random horizontal flips                | Yes                   |\n",
        "| Maximum degree of rotation             | 10Â°                   |\n",
        "| Minimum zoom                           | 1.0                   |\n",
        "| Maximum zoom                           | 1.1                   |\n",
        "| Maximum scale for changing brightness  | 0.2                   |\n",
        "| Maximum value for changing warp        | 0.2                   |\n",
        "| Probability of affine transformation   | 0.2                   |\n",
        "| Probability of brightness/contrast     | 0.75                  |\n",
        "| Padding mode                           | Reflection            |\n",
        "\n",
        "We can also see from the confusion matricies that the models perform very well regardless of the class imbalances in the training set. Class imbalance can be a major issue when training, due to the models prioritsing the classes with the most samples. If we had a dataset with 99% of one class, the model would in many cases disregard the other classes due to the small quantity. There are many ways to solve this though, where collecting more data, syntesising more data, changing the performance metric and penelising the model are possible options. This is not tested in this assignment, but should be included for future work.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbaa81c",
      "metadata": {},
      "source": [
        "## Final notebook of Paddy Disease Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zio2_mhhy-Mp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zio2_mhhy-Mp",
        "outputId": "6d9d1246-7b01-4490-9ebd-489a60c98c18"
      },
      "outputs": [],
      "source": [
        "!pip install fastkaggle fastai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67babdd9",
      "metadata": {
        "id": "67babdd9"
      },
      "outputs": [],
      "source": [
        "from fastkaggle import *\n",
        "from fastai.vision.all import *\n",
        "\n",
        "\n",
        "comp = 'paddy-disease-classification'\n",
        "\n",
        "path = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')\n",
        "path.ls()\n",
        "\n",
        "trn_path = path / \"train_images\"\n",
        "tst_path = path / \"test_images\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba329984",
      "metadata": {},
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ruYCZs1AfP77",
      "metadata": {
        "id": "ruYCZs1AfP77"
      },
      "outputs": [],
      "source": [
        "def train(arch, size, item=Resize(480, method='squish'), accum=1, finetune=True, epochs=12):\n",
        "    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item,\n",
        "        batch_tfms=aug_transforms(size=size, min_scale=0.75), bs=64//accum)\n",
        "    cbs = GradientAccumulation(64) if accum else []\n",
        "    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n",
        "    if finetune:\n",
        "        learn.fine_tune(epochs, 0.01)\n",
        "        learn.validate()\n",
        "        tst_files = get_image_files(tst_path)\n",
        "\n",
        "        interp = ClassificationInterpretation.from_learner(learn)\n",
        "        interp.plot_confusion_matrix()\n",
        "        interp.plot_top_losses(9)\n",
        "\n",
        "        test_dl = learn.dls.test_dl(tst_files)\n",
        "        return learn.tta(dl=test_dl)\n",
        "    else:\n",
        "        learn.unfreeze()\n",
        "        learn.fit_one_cycle(epochs, 0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a65ec256",
      "metadata": {},
      "source": [
        "### Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fQQ41BHIfh2S",
      "metadata": {
        "id": "fQQ41BHIfh2S"
      },
      "outputs": [],
      "source": [
        "res = 512,512\n",
        "models = {\n",
        "    'convnext_small_in22k': {\n",
        "        (Resize(res), 224),\n",
        "        (Resize(res), 299),\n",
        "        (Resize(res), 320),\n",
        "    }, 'vit_small_patch16_224': {\n",
        "         (Resize(res), 224),\n",
        "    } \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3vv-lnhFD_4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "a3vv-lnhFD_4",
        "outputId": "4362ddad-37fb-4b9a-9ed1-5b91963f0f98"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "tta_res = []\n",
        "\n",
        "for arch,details in models.items():\n",
        "    for item,size in details:\n",
        "        print('---',arch)\n",
        "        print(size)\n",
        "        print(item.name)\n",
        "        tta_res.append(train(arch, size, item=item, accum=2, epochs=10)) #, epochs=1))\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fdfa71f",
      "metadata": {},
      "source": [
        "### Output\n",
        "\n",
        "--- convnext_small_in22k\n",
        "224\n",
        "Resize -- {'size': (512, 512), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n",
        "\n",
        "\n",
        "| epoch | train_loss | valid_loss | error_rate |  time  |\n",
        "|-------|------------|------------|------------|--------|\n",
        "|   0   |  0.522924  |  0.291351  |  0.092744  | 01:07  |\n",
        "|   1   |  0.474965  |  0.308874  |  0.100913  | 01:04  |\n",
        "|   2   |  0.409522  |  0.312514  |  0.094185  | 01:05  |\n",
        "|   3   |  0.247534  |  0.196876  |  0.058626  | 01:04  |\n",
        "|   4   |  0.228018  |  0.183603  |  0.054301  | 01:04  |\n",
        "|   5   |  0.169902  |  0.132934  |  0.041326  | 01:05  |\n",
        "|   6   |  0.137842  |  0.120899  |  0.034599  | 01:05  |\n",
        "|   7   |  0.091253  |  0.113226  |  0.027871  | 01:05  |\n",
        "|   8   |  0.066507  |  0.107186  |  0.027871  | 01:04  |\n",
        "|   9   |  0.059744  |  0.105628  |  0.025949  | 01:03  |\n",
        "\n",
        "![Confusion matrix](images/convnext1.png)\n",
        "\n",
        "![Top 9 losses](images/convnext1_img.png)\n",
        "\n",
        "--- convnext_small_in22k\n",
        "299\n",
        "Resize -- {'size': (512, 512), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n",
        "\n",
        "| epoch | train_loss | valid_loss | error_rate |  time  |\n",
        "|-------|------------|------------|------------|--------|\n",
        "|   0   |  0.482487  |  0.301711  |  0.093224  | 01:33  |\n",
        "|   1   |  0.404623  |  0.259028  |  0.085055  | 01:31  |\n",
        "|   2   |  0.334706  |  0.276585  |  0.089861  | 01:31  |\n",
        "|   3   |  0.293459  |  0.220637  |  0.063431  | 01:32  |\n",
        "|   4   |  0.209628  |  0.169217  |  0.054301  | 01:36  |\n",
        "|   5   |  0.145868  |  0.137486  |  0.039404  | 01:37  |\n",
        "|   6   |  0.117963  |  0.117456  |  0.033157  | 01:36  |\n",
        "|   7   |  0.088826  |  0.099379  |  0.025949  | 01:35  |\n",
        "|   8   |  0.081176  |  0.086329  |  0.024027  | 01:37  |\n",
        "|   9   |  0.068929  |  0.085969  |  0.024027  | 01:35  |\n",
        "\n",
        "![Confusion matrix](images/convnext2.png)\n",
        "\n",
        "![Top 9 losses](images/convnext2_img.png)\n",
        "\n",
        "--- convnext_small_in22k\n",
        "320\n",
        "Resize -- {'size': (512, 512), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n",
        "\n",
        "| epoch | train_loss | valid_loss | error_rate |  time  |\n",
        "|-------|------------|------------|------------|--------|\n",
        "|   0   |  0.475270  |  0.282908  |  0.095627  | 05:25  |\n",
        "|   1   |  0.394569  |  0.250861  |  0.079769  | 05:33  |\n",
        "|   2   |  0.362648  |  0.222830  |  0.066314  | 05:53  |\n",
        "|   3   |  0.320339  |  0.183621  |  0.059106  | 05:57  |\n",
        "|   4   |  0.201428  |  0.160160  |  0.044690  | 05:56  |\n",
        "|   5   |  0.150571  |  0.149583  |  0.039404  | 05:56  |\n",
        "|   6   |  0.119630  |  0.112060  |  0.031235  | 05:56  |\n",
        "|   7   |  0.097585  |  0.100967  |  0.022105  | 05:53  |\n",
        "|   8   |  0.061469  |  0.091153  |  0.023546  | 05:44  |\n",
        "|   9   |  0.062056  |  0.088169  |  0.021624  | 05:44  |\n",
        "\n",
        "![Confusion matrix](images/convnext3.png)\n",
        "\n",
        "![Top 9 losses](images/convnext3_img.png)\n",
        "\n",
        "--- vit_small_patch16_224\n",
        "224\n",
        "Resize -- {'size': (512, 512), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n",
        "\n",
        "| epoch | train_loss | valid_loss | error_rate |  time  |\n",
        "|-------|------------|------------|------------|--------|\n",
        "|   0   |  0.629896  |  0.369405  |  0.114368  | 00:34  |\n",
        "|   1   |  0.473076  |  0.325375  |  0.100432  | 00:34  |\n",
        "|   2   |  0.472268  |  0.353715  |  0.104277  | 00:34  |\n",
        "|   3   |  0.342373  |  0.235656  |  0.069197  | 00:35  |\n",
        "|   4   |  0.280413  |  0.292275  |  0.080730  | 00:35  |\n",
        "|   5   |  0.225597  |  0.211669  |  0.057184  | 00:35  |\n",
        "|   6   |  0.145305  |  0.163454  |  0.038924  | 00:35  |\n",
        "|   7   |  0.135815  |  0.162330  |  0.038443  | 00:35  |\n",
        "|   8   |  0.086223  |  0.154728  |  0.032196  | 00:35  |\n",
        "|   9   |  0.091272  |  0.152397  |  0.032677  | 00:34  |\n",
        "\n",
        "![Confusion matrix](images/vit.png)\n",
        "\n",
        "![Top 9 losses](images/vit_img.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7Jtob0MpFFyM",
      "metadata": {
        "id": "7Jtob0MpFFyM"
      },
      "outputs": [],
      "source": [
        "save_pickle('tta_paddy.pkl', tta_res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WPwm_91TFpVI",
      "metadata": {
        "id": "WPwm_91TFpVI"
      },
      "outputs": [],
      "source": [
        "tta_prs = first(zip(*tta_res))\n",
        "avg_pr = torch.stack(tta_prs).mean(0)\n",
        "dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(480, method='squish'),\n",
        "    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n",
        "\n",
        "tst_files = get_image_files(tst_path)\n",
        "\n",
        "idxs = avg_pr.argmax(dim=1)\n",
        "vocab = np.array(dls.vocab)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"image_id\": [f.name for f in tst_files.items],\n",
        "    \"label\": vocab[idxs]\n",
        "})\n",
        "submission.to_csv(\"paddy-submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8SS6zi8e43o3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SS6zi8e43o3",
        "outputId": "d8e18b3a-c72c-46ed-aa27-332e8bea1e78"
      },
      "outputs": [],
      "source": [
        "if not iskaggle:\n",
        "    from kaggle import api\n",
        "    api.competition_submit_cli('paddy-submission.csv', 'ensemble 512px 10e', comp)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

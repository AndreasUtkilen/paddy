{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f56e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/paddy/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "train_path = kagglehub.dataset_download(\"shreyansjain04/ai-vs-real-image-dataset\")\n",
    "\n",
    "test_path = kagglehub.dataset_download(\"shreyansjain04/ai-vs-real-image-test-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14350dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_ct(image, kernel_size=3, max_iter=10):\n",
    "    def em_channel(channel):\n",
    "        alpha = kernel_size // 2\n",
    "        padded = cv2.copyMakeBorder(channel, alpha, alpha, alpha, alpha, cv2.BORDER_REFLECT)\n",
    "        h, w = channel.shape\n",
    "        N = h * w\n",
    "        d = kernel_size**2 - 1\n",
    "\n",
    "        patch_offsets = []\n",
    "        center = kernel_size // 2\n",
    "        for i in range(kernel_size):\n",
    "            for j in range(kernel_size):\n",
    "                if i == center and j == center:\n",
    "                    continue\n",
    "                patch_offsets.append((i - center, j - center))\n",
    "\n",
    "        A = np.zeros((N, d), dtype=np.float32)\n",
    "        b = np.zeros(N, dtype=np.float32)\n",
    "\n",
    "        idx = 0\n",
    "        for y in range(alpha, h + alpha):\n",
    "            for x in range(alpha, w + alpha):\n",
    "                A[idx] = [padded[y + dy, x + dx] for dy, dx in patch_offsets]\n",
    "                b[idx] = padded[y, x]\n",
    "                idx += 1\n",
    "\n",
    "        k = np.zeros(d, dtype=np.float32)\n",
    "        for _ in range(max_iter):\n",
    "            pred = A @ k\n",
    "            residuals = b - pred\n",
    "            sigma2 = np.mean(residuals**2)\n",
    "            weights = np.exp(-residuals**2 / (2 * sigma2))\n",
    "\n",
    "            Aw = A * weights[:, np.newaxis]\n",
    "            bw = b * weights\n",
    "            k = np.linalg.pinv(A.T @ Aw) @ (A.T @ bw)\n",
    "\n",
    "        return k\n",
    "\n",
    "    if image.shape[2] != 3:\n",
    "        raise ValueError(\"Image must be RGB\")\n",
    "\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    return np.concatenate([em_channel(image[..., c]) for c in range(3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f07bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 8837/48096 [2:33:28<12:36:58,  1.16s/it]/tmp/ipykernel_1411/1378728879.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  weights = np.exp(-residuals**2 / (2 * sigma2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PILImage mode=RGB size=128x128\n",
      "Image not RGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48096/48096 [13:50:59<00:00,  1.04s/it]  \n",
      " 12%|█▏        | 1449/12024 [25:02<2:26:42,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image not RGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 8150/12024 [2:22:15<1:07:37,  1.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m tqdm(dls.valid_ds):\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         ct_vec = \u001b[43mextract_ct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m         X_valid.append(ct_vec)\n\u001b[32m     56\u001b[39m         y_valid.append(\u001b[38;5;28mint\u001b[39m(label))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mextract_ct\u001b[39m\u001b[34m(image, kernel_size, max_iter)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mImage must be RGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m image = image.astype(np.float32) / \u001b[32m255.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.concatenate([\u001b[43mem_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m)])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mextract_ct.<locals>.em_channel\u001b[39m\u001b[34m(channel)\u001b[39m\n\u001b[32m     27\u001b[39m         A[idx] = [padded[y + dy, x + dx] \u001b[38;5;28;01mfor\u001b[39;00m dy, dx \u001b[38;5;129;01min\u001b[39;00m patch_offsets]\n\u001b[32m     28\u001b[39m         b[idx] = padded[y, x]\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m         idx += \u001b[32m1\u001b[39m\n\u001b[32m     31\u001b[39m k = np.zeros(d, dtype=np.float32)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from pathlib import Path\n",
    "\n",
    "def get_balanced_subset(path, max_per_class=500):\n",
    "    files = get_image_files(path)\n",
    "    grouped = {}\n",
    "    for f in files:\n",
    "        lbl = parent_label(f).lower()\n",
    "        grouped.setdefault(lbl, []).append(f)\n",
    "\n",
    "    selected = []\n",
    "    for lbl, f_list in grouped.items():\n",
    "        selected.extend(f_list[:max_per_class])\n",
    "    \n",
    "    return selected\n",
    "\n",
    "path = Path(\"sml\")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=RandomSplitter(seed=42)\n",
    ")\n",
    "\n",
    "dls = dblock.dataloaders(path, bs=16)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "i = 0\n",
    "X, y = [], []\n",
    "for img, label in tqdm(dls.train_ds):\n",
    "    try:\n",
    "        ct_vec = extract_ct(np.array(img), kernel_size=3)\n",
    "        X.append(ct_vec)\n",
    "        y.append(int(label))\n",
    "    except ValueError:\n",
    "        print(img)\n",
    "        print(\"Image not RGB\")\n",
    "\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        np.save(f\"ct_vectors_{i}.npy\", X)\n",
    "\n",
    "X_valid, y_valid = [], []\n",
    "for img, label in tqdm(dls.valid_ds):\n",
    "    try:\n",
    "        ct_vec = extract_ct(np.array(img), kernel_size=3)\n",
    "        X_valid.append(ct_vec)\n",
    "        y_valid.append(int(label))\n",
    "    except ValueError:\n",
    "        print(\"Image not RGB\")\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "y_pred = rf.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"ai\", \"real\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

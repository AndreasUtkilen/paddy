{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7ed4519",
      "metadata": {},
      "source": [
        "## Task 2 - Deepfake Detection {-}\n",
        "\n",
        "In this task, we will create a model to detect deepfake images using a dataset of real and fake images. Similar to the paddy disease detection task, we will leverage transfer learning and fine-tune a pre-trained model for this purpose.\n",
        "\n",
        "### Initial Experiments: ResNet50 Baseline {-}\n",
        "I began by analyzing the dataset and applied similar preprocessing techniques as used in the paddy project. For our baseline, I selected the ResNet50 architecture due to its proven performance and fast training capabilities, especially in early prototyping. By keeping the number of training epochs low, I could quickly iterate over various configurations of image size, augmentations, and learning rates.\n",
        "\n",
        "To determine the optimal input resolution, I conducted experiments with image sizes of 512×512, 256×256, and 128×128. Surprisingly, I observed that smaller image sizes often yielded better performance, likely due to the model’s improved ability to generalize. However, reducing the resolution too much caused the model to miss subtle artifacts and imperfections characteristic of deepfakes. Ultimately, the 256×256 resolution struck a balance between detail and generalization, producing the best results for ResNet50. This model achieved a Kaggle score of approximately 0.895, which served as a strong baseline.\n",
        "\n",
        "| epoch | train_loss | valid_loss | error_rate | time  |\n",
        "|-------|------------|------------|------------|-------|\n",
        "| 0     | 0.193611   | 0.365707   | 0.112774   | 03:11 |\n",
        "| 1     | 0.155723   | 0.326862   | 0.102295   | 03:11 |\n",
        "| 2     | 0.125915   | 0.298038   | 0.095060   | 03:14 |\n",
        "| 3     | 0.133721   | 0.297405   | 0.100882   | 03:19 |\n",
        "\n",
        "![Confusion matrix for resnet50](images/resnet50_cm.png)\n",
        "\n",
        "![Prediction/loss for resnet50](images/pred_loss.png)\n",
        "\n",
        "*Figure: Images show confusion matrix and top losses for the best ResNet50 model*\n",
        "\n",
        "An interesting thing about the confusion matrix for the ResNet model is that it does not look very good. There is a lot of wrongly predicted images and especially when the image is real, but the model predict ai. Regardless of this not so good confusion matrix, it produces very good Kaggle scores, which tells me that the other models that I have tested likely have overfitted heavily on the dataset. The overfitting, however, seems also a bit strange as the dataset is quite large and I usually only run the models for a few epochs.\n",
        "\n",
        "### Testing Alternative Architectures {-}\n",
        "I then explored other architectures such as ConvNeXt Small and Vision Transformers (ViT), hypothesizing that their ability to model local textures could offer an advantage in detecting subtle deepfake imperfections. Multiple configurations were tested, including different patch sizes and training schedules.\n",
        "\n",
        "Although the ConvNeXt Small model performed comparably well, reaching a score of 0.889, it required significantly more computational resources and longer training time without offering a meaningful improvement. Similarly, ViT models showed promise but consistently fell short of outperforming ResNet50. These results suggest that for this particular task, ResNet50 strikes a favorable tradeoff between accuracy, training speed, and generalization.\n",
        "\n",
        "### Ensemble Strategy {-}\n",
        "As in the paddy disease detection task, I attempted to ensemble multiple models with ResNet50, ConvNeXt Small, and ViT variants—to combine their strengths. Ensembling is generally expected to reduce variance and leverage complementary decision boundaries, leading to improved performance.\n",
        "\n",
        "However, in this case, the ensemble approach did not lead to an improvement over the best single ResNet50 model. The averaged predictions showed minor fluctuations in accuracy and even underperformed slightly compared to the standalone ResNet50. Several factors may explain this; Firstly, rhe individual models may have learned similar decision boundaries, reducing the benefit of ensembling. Secondly, averaging softmax probabilities across models with different confidence levels might have introduced noise. And lastly, models like ViT and ConvNeXt, though powerful, might fail to pick up on the exact set of features critical for this task, weakening the ensemble’s consensus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zio2_mhhy-Mp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zio2_mhhy-Mp",
        "outputId": "688c3215-53f2-4a25-a6d5-cde8fa1ef4bd"
      },
      "outputs": [],
      "source": [
        "!pip install fastkaggle fastai kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67babdd9",
      "metadata": {
        "id": "67babdd9"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "\n",
        "from fastkaggle import *\n",
        "\n",
        "comp = 'hack-rush-deep-fake-detection'\n",
        "\n",
        "path = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51e0e9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b51e0e9d",
        "outputId": "97757384-e0a6-4d81-9193-9fdf40a2e2e0"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "train_path = kagglehub.dataset_download(\"shreyansjain04/ai-vs-real-image-dataset\")\n",
        "\n",
        "test_path = kagglehub.dataset_download(\"shreyansjain04/ai-vs-real-image-test-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d80b1eaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d80b1eaa",
        "outputId": "7ea94e2c-c377-42b8-a262-a8bb0b9f690d"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "\n",
        "trn_path = Path('mic')\n",
        "tst_path = Path('mic-test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a861cfa1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a861cfa1",
        "outputId": "2361c99b-bf10-4596-8be1-c4363004bf94"
      },
      "outputs": [],
      "source": [
        "resize_images(train_path, dest=trn_path, max_size=128, recurse=True, max_workers=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c7c5f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "resize_images(test_path, dest=tst_path, max_size=128, recurse=True, max_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d84b0ee7",
      "metadata": {},
      "source": [
        "### Train function {-}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ruYCZs1AfP77",
      "metadata": {
        "id": "ruYCZs1AfP77"
      },
      "outputs": [],
      "source": [
        "def train(arch, size, item=Resize(480, method='squish'), accum=1, finetune=True, epochs=12):\n",
        "    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item, batch_tfms=aug_transforms(size=size, min_scale=0.75), bs=64//accum)\n",
        "    cbs = GradientAccumulation(64) if accum else []\n",
        "    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n",
        "    if finetune:\n",
        "        learn.fine_tune(epochs, 0.01)\n",
        "        learn.export(f\"{arch}_{size}_e{epochs}\")\n",
        "        tst_files = get_image_files(tst_path)\n",
        "\n",
        "        interp = ClassificationInterpretation.from_learner(learn)\n",
        "        interp.plot_confusion_matrix()\n",
        "        interp.plot_top_losses(9)\n",
        "\n",
        "        test_dl = learn.dls.test_dl(tst_files)\n",
        "        preds, _ = learn.tta(dl=test_dl)\n",
        "\n",
        "        submission = pd.DataFrame({\n",
        "            \"filename\": [f.name for f in test_dl.items],\n",
        "            \"class\": preds.argmax(dim=1).numpy()\n",
        "        })\n",
        "        submission.to_csv(\"submission.csv\", index=False)\n",
        "        return learn\n",
        "    else:\n",
        "        learn.unfreeze()\n",
        "        learn.fit_one_cycle(epochs, 0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1645d617",
      "metadata": {},
      "source": [
        "### Ensemble functions {-}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e551d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "res = 128,128\n",
        "models = {\n",
        "    'convnext_small_in22k': {\n",
        "        (Resize((128, 128)), 224),\n",
        "        (Resize((256, 256)), 299),\n",
        "        (Resize((128, 128)), 320),\n",
        "    }, \n",
        "    'vit_small_patch16_224': {\n",
        "         (Resize((128, 128)), 224),\n",
        "         (Resize((256, 256)), 224),\n",
        "    } \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe510944",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "tta_res = np.load(\"tta_res.npy\")\n",
        "\n",
        "for arch,details in models.items():\n",
        "    for item,size in details:\n",
        "        print('---',arch)\n",
        "        print(size)\n",
        "        print(item.name)\n",
        "        tta_res = np.append(tta_res, train(arch, size, item=item, accum=2, epochs=10)) #, epochs=1))\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b209309",
      "metadata": {},
      "outputs": [],
      "source": [
        "tta_prs = first(zip(*tta_res))\n",
        "avg_pr = torch.stack(tta_prs).mean(0)\n",
        "dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(480, method='squish'),\n",
        "    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n",
        "\n",
        "tst_files = get_image_files(tst_path)\n",
        "\n",
        "idxs = avg_pr.argmax(dim=1)\n",
        "vocab = np.array(dls.vocab)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"image_id\": [f.name for f in tst_files.items],\n",
        "    \"label\": vocab[idxs]\n",
        "})\n",
        "submission.to_csv(\"deepfake-submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "059b3370",
      "metadata": {},
      "source": [
        "### ResNet50 Model {-}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2032862",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "learn = train('resnet50', 256, item=Resize((128, 128)), accum=1, epochs=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8SS6zi8e43o3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SS6zi8e43o3",
        "outputId": "d8e18b3a-c72c-46ed-aa27-332e8bea1e78"
      },
      "outputs": [],
      "source": [
        "if not iskaggle:\n",
        "    from kaggle import api\n",
        "    api.competition_submit_cli('submission.csv', 'resnet50 128 6e', comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d36749",
      "metadata": {},
      "source": [
        "### Other methods from the literature {-}\n",
        "\n",
        "After researching the topic, I found a paper called \"Fighting deepfake by exposing the convolutional traces on images\" (Guarnera et al., 2020) that suggested using a technique were you calculate convolution traces of the images, by using a algorithm called Expectation Maximization (EM). The method works by calculating the convolution traces of the images, which are the tiny patterns, imperfections and artifacts left behind by the GANs (Generative Adversarial Networks) that create deepfakes. These traces are often too subtle for the human eye to detect, but they can be captured by the EM algorithm. The EM algorithm iteratively refines the estimates of the convolution traces until they converge to a stable solution. Once the traces are calculated, they can be used as features for a classifier, such as Random Forest, to distinguish between real and fake images. A simple implementation of this algorithm was tested, and by using the Random Forest classifier, as recommended in the paper, I managed to get a accuracy of around 0.6, which is not particularly good. I therefore tried to use Support Vector Classification (SVC) as an alternative to Random Forest classifier. This increased the accuracy to 0.7, and shows that the method works, as the computed convolution traces have some signal, but they are noisy and hard to classify. In the paper, they get an even higher accuracy at around 0.9 and above, which tells me that my implementation of the paper might not be as good as theirs, and without GPU acceleration of the convolution trace computation it takes many hours to finish the dataset. \n",
        "\n",
        "Looking at this solution it is quite nice to see that the task is solvable without needing a black box, which these CNNs and RNNs are, and it is possible to explain how the model made its prediction on a understandable level. Making an understandable model will be important to be able to gain trust in the model and will supersede any deep learning model with equal accuracy.\n",
        "\n",
        "Another paper titled \"Deepfake Detection and Classification of Images from Video: A Review of Features, Techniques, and Challenges\" (Bale, et al. 2024), gives a structured review of how deepfake images, especially those extracted from video, are detected and classified and it outlines three main approaches.\n",
        "\n",
        "Feature-based methods rely on spotting visual inconsistencies like unnatural lighting, irregular eye reflections, or distorted facial expressions. Traditional machine learning models use predefined features to train classifiers, but they often struggle to adapt to new types of deepfakes. Deep learning techniques, and especially convolutional neural networks (CNNs) are more robust, as they can learn complex features and adapt to small manipulations.\n",
        "\n",
        "The paper points out several ongoing challenges in the field, such as the rapid evolution of deepfake creation tools, the limited availability of diverse and representative datasets, and the difficulty of building models that perform well across different scenarios. To address these, the authors propose a framework for comparing detection techniques and stress the importance of real-world applicability, model robustness, and future research aimed at staying ahead of increasingly realistic forgeries.\n",
        "\n",
        "Guarnera, L., Giudice, O., & Battiato, S. (2020). Fighting deepfake by exposing the convolutional traces on images. IEEE access, 8, 165085-165098.\n",
        "\n",
        "Bale, D. L. T., Ochei, L. C., & Ugwu, C. (2024). Deepfake Detection and Classification of Images from Video: A Review of Features, Techniques, and Challenges. International Journal of Intelligent Information Systems, 13(2), 17–27. https://doi.org/10.11648/j.ijiis.20241302.11"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e28122",
      "metadata": {},
      "source": [
        "### Convolutional traces {-}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a21183f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def extract_ct(image, kernel_size=3, max_iter=10):\n",
        "    def em_channel(channel):\n",
        "        alpha = kernel_size // 2\n",
        "        padded = cv2.copyMakeBorder(channel, alpha, alpha, alpha, alpha, cv2.BORDER_REFLECT)\n",
        "        h, w = channel.shape\n",
        "        N = h * w\n",
        "        d = kernel_size**2 - 1\n",
        "\n",
        "        patch_offsets = []\n",
        "        center = kernel_size // 2\n",
        "        for i in range(kernel_size):\n",
        "            for j in range(kernel_size):\n",
        "                if i == center and j == center:\n",
        "                    continue\n",
        "                patch_offsets.append((i - center, j - center))\n",
        "\n",
        "        A = np.zeros((N, d), dtype=np.float32)\n",
        "        b = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "        idx = 0\n",
        "        for y in range(alpha, h + alpha):\n",
        "            for x in range(alpha, w + alpha):\n",
        "                A[idx] = [padded[y + dy, x + dx] for dy, dx in patch_offsets]\n",
        "                b[idx] = padded[y, x]\n",
        "                idx += 1\n",
        "\n",
        "        k = np.zeros(d, dtype=np.float32)\n",
        "        for _ in range(max_iter):\n",
        "            pred = A @ k\n",
        "            residuals = b - pred\n",
        "            sigma2 = np.mean(residuals**2)\n",
        "            weights = np.exp(-residuals**2 / (2 * sigma2))\n",
        "\n",
        "            Aw = A * weights[:, np.newaxis]\n",
        "            bw = b * weights\n",
        "            k = np.linalg.pinv(A.T @ Aw) @ (A.T @ bw)\n",
        "\n",
        "        return k\n",
        "\n",
        "    if image.shape[2] != 3:\n",
        "        raise ValueError(\"Image must be RGB\")\n",
        "\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    return np.concatenate([em_channel(image[..., c]) for c in range(3)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33efe744",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from fastai.vision.all import *\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from pathlib import Path\n",
        "\n",
        "def get_balanced_subset(path, max_per_class=500):\n",
        "    files = get_image_files(path)\n",
        "    grouped = {}\n",
        "    for f in files:\n",
        "        lbl = parent_label(f).lower()\n",
        "        grouped.setdefault(lbl, []).append(f)\n",
        "\n",
        "    selected = []\n",
        "    for lbl, f_list in grouped.items():\n",
        "        selected.extend(f_list[:max_per_class])\n",
        "    \n",
        "    return selected\n",
        "\n",
        "path = Path(\"sml\")\n",
        "\n",
        "dblock = DataBlock(\n",
        "    blocks=(ImageBlock, CategoryBlock),\n",
        "    get_items=get_image_files,\n",
        "    get_y=parent_label,\n",
        "    splitter=RandomSplitter(seed=42)\n",
        ")\n",
        "\n",
        "dls = dblock.dataloaders(path, bs=16)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "i = 0\n",
        "X, y = [], []\n",
        "for img, label in tqdm(dls.train_ds):\n",
        "    try:\n",
        "        ct_vec = extract_ct(np.array(img), kernel_size=3)\n",
        "        X.append(ct_vec)\n",
        "        y.append(int(label))\n",
        "    except ValueError:\n",
        "        print(img)\n",
        "        print(\"Image not RGB\")\n",
        "\n",
        "    i += 1\n",
        "    if i % 10000 == 0:\n",
        "        np.save(f\"ct_vectors_{i}.npy\", X)\n",
        "\n",
        "X_valid, y_valid = [], []\n",
        "for img, label in tqdm(dls.valid_ds):\n",
        "    try:\n",
        "        ct_vec = extract_ct(np.array(img), kernel_size=3)\n",
        "        X_valid.append(ct_vec)\n",
        "        y_valid.append(int(label))\n",
        "    except ValueError:\n",
        "        print(\"Image not RGB\")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "y_pred = rf.predict(X_valid)\n",
        "print(classification_report(y_valid, y_pred, target_names=[\"ai\", \"real\"]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
